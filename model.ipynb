{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy==2.2.3 in /home/blank/.local/lib/python3.8/site-packages (2.2.3)\n",
      "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (1.0.6)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (2.0.7)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (0.4.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (1.0.9)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (2.28.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (3.0.8)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (1.1.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (0.10.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from spacy==2.2.3) (45.2.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (1.24.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (1.0.2)\n",
      "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /home/blank/.local/lib/python3.8/site-packages (from spacy==2.2.3) (7.3.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/blank/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (3.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/blank/.local/lib/python3.8/site-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.3) (2019.11.28)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /home/blank/.local/lib/python3.8/site-packages (from thinc<7.4.0,>=7.3.0->spacy==2.2.3) (4.64.1)\n",
      "/bin/bash: python: command not found\n",
      "Requirement already satisfied: beautifulsoup4==4.9.1 in /home/blank/.local/lib/python3.8/site-packages (4.9.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in /usr/lib/python3/dist-packages (from beautifulsoup4==4.9.1) (1.9.5)\n",
      "Requirement already satisfied: textblob==0.15.3 in /home/blank/.local/lib/python3.8/site-packages (0.15.3)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/blank/.local/lib/python3.8/site-packages (from textblob==0.15.3) (3.8.1)\n",
      "Requirement already satisfied: joblib in /home/blank/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob==0.15.3) (1.2.0)\n",
      "Requirement already satisfied: tqdm in /home/blank/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob==0.15.3) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/blank/.local/lib/python3.8/site-packages (from nltk>=3.1->textblob==0.15.3) (2023.3.23)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk>=3.1->textblob==0.15.3) (7.0)\n",
      "Collecting git+https://github.com/laxmimerit/preprocess_kgptalkie.git\n",
      "  Cloning https://github.com/laxmimerit/preprocess_kgptalkie.git to /tmp/pip-req-build-eugibhnd\n",
      "  Running command git clone -q https://github.com/laxmimerit/preprocess_kgptalkie.git /tmp/pip-req-build-eugibhnd\n",
      "Building wheels for collected packages: preprocess-kgptalkie\n",
      "  Building wheel for preprocess-kgptalkie (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for preprocess-kgptalkie: filename=preprocess_kgptalkie-0.1.3-py3-none-any.whl size=7633 sha256=13128a83d343f74486103992069246c8b45298771ff37b32ef324e4ef3605d43\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-wwvmbwqz/wheels/fd/71/47/e04f208ad659a295ccb69022e14de7d20ccc6fc30a5a935f09\n",
      "Successfully built preprocess-kgptalkie\n",
      "Installing collected packages: preprocess-kgptalkie\n",
      "  Attempting uninstall: preprocess-kgptalkie\n",
      "    Found existing installation: preprocess-kgptalkie 0.1.3\n",
      "    Uninstalling preprocess-kgptalkie-0.1.3:\n",
      "      Successfully uninstalled preprocess-kgptalkie-0.1.3\n",
      "Successfully installed preprocess-kgptalkie-0.1.3\n"
     ]
    }
   ],
   "source": [
    "# Dependencies\n",
    "\n",
    "!pip install spacy==2.2.3\n",
    "!python -m spacy download en_core_web_sm\n",
    "!pip install beautifulsoup4==4.9.1\n",
    "!pip install textblob==0.15.3\n",
    "!pip install git+https://github.com/laxmimerit/preprocess_kgptalkie.git --upgrade --force-reinstall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import nltk\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import LSTM, Dropout, Dense, Embedding\n",
    "from keras import Sequential\n",
    "import preprocess_kgptalkie as ps\n",
    "from preprocess import preprocess_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
       "\n",
       "                                                text subject  \\\n",
       "0  Donald Trump just couldn t wish all Americans ...    News   \n",
       "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
       "2  On Friday, it was revealed that former Milwauk...    News   \n",
       "3  On Christmas day, Donald Trump announced that ...    News   \n",
       "4  Pope Francis used his annual Christmas Day mes...    News   \n",
       "\n",
       "                date  real  \n",
       "0  December 31, 2017     0  \n",
       "1  December 31, 2017     0  \n",
       "2  December 30, 2017     0  \n",
       "3  December 29, 2017     0  \n",
       "4  December 25, 2017     0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the 1st dataset\n",
    "\n",
    "df_real = pd.read_csv('data/True.csv')\n",
    "df_fake = pd.read_csv('data/Fake.csv')\n",
    "df_real['real'] = 1\n",
    "df_fake['real'] = 0\n",
    "df_1 = pd.concat([df_fake, df_real], axis=0)\n",
    "df_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>real</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>muslims busted they stole millions in govt ben...</td>\n",
       "      <td>print they should pay all the back all the mon...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>re why did attorney general loretta lynch plea...</td>\n",
       "      <td>why did attorney general loretta lynch plead t...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>breaking weiner cooperating with fbi on hillar...</td>\n",
       "      <td>red state  \\nfox news sunday reported this mor...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pin drop speech by father of daughter kidnappe...</td>\n",
       "      <td>email kayla mueller was a prisoner and torture...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fantastic trumps  point plan to reform healthc...</td>\n",
       "      <td>email healthcare reform to make america great ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  muslims busted they stole millions in govt ben...   \n",
       "1  re why did attorney general loretta lynch plea...   \n",
       "2  breaking weiner cooperating with fbi on hillar...   \n",
       "3  pin drop speech by father of daughter kidnappe...   \n",
       "4  fantastic trumps  point plan to reform healthc...   \n",
       "\n",
       "                                                text  real  \n",
       "0  print they should pay all the back all the mon...   1.0  \n",
       "1  why did attorney general loretta lynch plead t...   1.0  \n",
       "2  red state  \\nfox news sunday reported this mor...   1.0  \n",
       "3  email kayla mueller was a prisoner and torture...   1.0  \n",
       "4  email healthcare reform to make america great ...   1.0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the 2nd dataset\n",
    "\n",
    "df_2 = pd.read_csv('data/news_articles.csv')\n",
    "df_2['real'] = df_2['label'].replace({'Real':1,'Fake':0})\n",
    "df_2 = df_2.dropna(axis=0)\n",
    "df_2 = pd.DataFrame(df_2[['title','text', 'real']])\n",
    "df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a tokenizer with the preprocessed dataset\n",
    "\n",
    "df = pd.read_csv(\"data/preprocessed_data.csv\")\n",
    "x = [row.split() for row in df.text.tolist()]\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898, 1000) (2045, 1000) (46943, 1000)\n"
     ]
    }
   ],
   "source": [
    "# Defining Features after preprocessing\n",
    "\n",
    "X_1 = preprocess_data(df_1, tokenizer=tokenizer)\n",
    "X_2 = preprocess_data(df_2, tokenizer=tokenizer)\n",
    "X = np.concatenate((X_1, X_2))\n",
    "print(X_1.shape, X_2.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44898,) (2045,) (46943,)\n"
     ]
    }
   ],
   "source": [
    "# Defining output label for the dataset\n",
    "\n",
    "Y_1 = df_1.real.values\n",
    "Y_2 = df_2.real.values\n",
    "Y = np.concatenate((Y_1, Y_2))\n",
    "print(Y_1.shape, Y_2.shape, Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word Embedding \n",
    "word2vec = gensim.models.Word2Vec(sentences=x, window=5, min_count = 1)\n",
    "def get_weights(model):\n",
    "    weights = np.zeros(((len(tokenizer.word_index) + 1), 100))\n",
    "\n",
    "    for word, i in tokenizer.word_index.items():\n",
    "        weights[i] = word2vec.wv[word]\n",
    "    return weights\n",
    "embedding_vectors = get_weights(word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 100)         37537400  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 128)               117248    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               33024     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 37,687,929\n",
      "Trainable params: 150,529\n",
      "Non-trainable params: 37,537,400\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    Embedding((len(tokenizer.word_index) + 1), output_dim = 100, weights=[embedding_vectors], input_length = 1000, trainable=False),\n",
    "    LSTM(units = 128),\n",
    "    Dropout(0.2),\n",
    "    Dense(256),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['acc'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the validation set \n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "771/771 [==============================] - 474s 612ms/step - loss: 0.1652 - acc: 0.9378 - val_loss: 0.1107 - val_acc: 0.9591\n",
      "Epoch 2/6\n",
      "771/771 [==============================] - 463s 600ms/step - loss: 0.0980 - acc: 0.9662 - val_loss: 0.0628 - val_acc: 0.9773\n",
      "Epoch 3/6\n",
      "771/771 [==============================] - 478s 620ms/step - loss: 0.0709 - acc: 0.9743 - val_loss: 0.1394 - val_acc: 0.9519\n",
      "Epoch 4/6\n",
      "771/771 [==============================] - 480s 622ms/step - loss: 0.0593 - acc: 0.9787 - val_loss: 0.0532 - val_acc: 0.9822\n",
      "Epoch 5/6\n",
      "771/771 [==============================] - 457s 593ms/step - loss: 0.0473 - acc: 0.9833 - val_loss: 0.0646 - val_acc: 0.9768\n",
      "Epoch 6/6\n",
      "771/771 [==============================] - 455s 591ms/step - loss: 0.0471 - acc: 0.9833 - val_loss: 0.0522 - val_acc: 0.9824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21de59a6af0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Training the model \n",
    "\n",
    "model.fit(x_train, y_train, validation_split=0.3, epochs=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 [==============================] - 63s 170ms/step\n",
      "score: 0.9816802999318337\n"
     ]
    }
   ],
   "source": [
    "predictions = (model.predict(x_test) >= 0.5).astype(int)\n",
    "print('score:', accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "# saving an image of the model in the form of a plot\n",
    "\n",
    "tf.keras.utils.plot_model(\n",
    "    model,\n",
    "    to_file='model.png',\n",
    "    show_shapes=False,\n",
    "    show_dtype=False,\n",
    "    show_layer_names=True,\n",
    "    rankdir='TB',\n",
    "    expand_nested=False,\n",
    "    dpi=96,\n",
    "    layer_range=None,\n",
    "    show_layer_activations=False,\n",
    "    show_trainable=False\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "020de65836c42a6812258e5bca9944a6536f5ab2105a429558de7e15e2b132ca"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
